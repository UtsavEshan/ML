{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee50fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7c9105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text:1115394 characters\n"
     ]
    }
   ],
   "source": [
    "#Getting Data\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text:{len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7cb50f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea62482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text pre processing\n",
    "#Need to convert text to a numerical representation\n",
    "#Makes each character into numeric ID\n",
    "\n",
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None) #defined a layer to do conversion\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None) #To get letters from ids\n",
    "\n",
    "\n",
    "#Define a function to join the characters to make a string\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f16f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data labelling\n",
    "#the input is contain seq_length # of characters, and so does the target\n",
    "#therefore break into chunks of seq_length+1\n",
    "\n",
    "#convert text vector into a stream of character indices\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text,'UTF-8'))\n",
    "seq_length=100\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#mapping to (input,label) pairs\n",
    "def split_input_target(sequence):\n",
    "    input_text=sequence[:-1]\n",
    "    target_text=sequence[1:]\n",
    "    return input_text,target_text\n",
    "\n",
    "dataset=sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd844b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BATCHING\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE= 1000 #buffer in which it shuffles element\n",
    "dataset=(\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f62a10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "\n",
    "vocab_size=len(ids_from_chars.get_vocabulary())\n",
    "embedding_dim=256\n",
    "rnn_units=1024\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a39b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55840a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "621e274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 5s 22ms/step - loss: 2.7057\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 4s 21ms/step - loss: 1.9952\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.7271\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 4s 21ms/step - loss: 1.5607\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 4s 21ms/step - loss: 1.4551\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.3840\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.3289\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 4s 21ms/step - loss: 1.2827\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.2393\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.1978\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.1563\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.1128\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.0663\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 1.0187\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 0.9675\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 0.9151\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 0.8639\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 0.8148\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 0.7676\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 4s 22ms/step - loss: 0.7265\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8e821a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Three in another seful.\n",
      "Hath not been in me; and she knave is his.\n",
      "\n",
      "GLOUCESTER:\n",
      "What ever I acquice the meat, Pomfort; no harm?\n",
      "\n",
      "GREMIO:\n",
      "I am done, or else will I be your own.\n",
      "\n",
      "ALONSO:\n",
      "But they are soldier, as they do,\n",
      "I would they desire them upon an infringed womp;\n",
      "'Tis but here pounday, and a bale in fantasticat,\n",
      "O God, thou shalt let her beseeming her,\n",
      "She for such venwees to the appeal, back'd and\n",
      "one, I would hang him. But, out on our house,\n",
      "By heather for your excellant potrock:\n",
      "Holder, farewell; one whoreson, thou shouldst\n",
      "be patripious at the irland. On he did but ever\n",
      "so hardly as heaven, by any of this once.\n",
      "\n",
      "LEONTES:\n",
      "Thou art to hear.\n",
      "Can I beseech you, Kate: none care you off.\n",
      "Here in Vienna, sir, but us in all: but hath\n",
      "cerved'd me his hands\n",
      "Out of this good and sit in all;\n",
      "Of where he stands too burn a hoal!\n",
      "Moved. What elce, I have overheeld you.\n",
      "\n",
      "VOLUMNIA:\n",
      "I would do thee this roar file week:\n",
      "This fellow I the dear'st o' both of weather.\n",
      "Courage all agree, do Ingo our \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.932492971420288\n"
     ]
    }
   ],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states\n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff7cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
