{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa47a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "import csv\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm   \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from itertools import product    \n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import dates as mpl_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def plot_series(time, series, col = 'dodgerblue', lab = 'original', format=\"-\", start=0, end=None):\n",
    "    plt.style.use('seaborn')\n",
    "    plt.plot(time[start:end], series[start:end], format, color = col, label = lab)\n",
    "    # display the grid\n",
    "    plt.grid(True)\n",
    "    plt.gcf().autofmt_xdate() \n",
    "    # set the format to out x-axis, gca is the get current axis\n",
    "    plt.gca().xaxis.set_major_formatter(date_formate)\n",
    "    plt.tight_layout() \n",
    "    plt.legend(loc = 'best')\n",
    "    \n",
    "    \n",
    "df = pd.read_csv('file name here')\n",
    "del df['Unnamed: 0']\n",
    "df.head()\n",
    "\n",
    "#results - univariate time series problems, no obvious trend, no white noise, 11 year seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e75314",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_time = int(len(time_step)*0.8)\n",
    "time_train = time_step[:split_time]\n",
    "x_train = sunspots[:split_time]\n",
    "time_valid = time_step[split_time:]\n",
    "x_valid = sunspots[split_time:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688fa933",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_time = int(len(time_step)*0.8)\n",
    "train = timeseries[:split_time]\n",
    "valid = timeseries[split_time:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "#NOTE: Statistics and other methods  were done and removed -- ARIMA, Moving average and naive forecast were used\n",
    "\n",
    "#simple DNN setup with windowed dataset to train data\n",
    "window_size = 60\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "# turn a series into a dataset which we can train on\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    # create a dataset from series\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    # slice the data up into appropriate windows\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    # flattened into chunks in the size of our window size + 1\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    # batched into the selected batch size and returned\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "# three layer of 20,10 and 1 neurons, input shape is the size of window \n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, input_shape=[window_size], activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "#MAE used to penalize large errors instead of MSE (need to check with both)\n",
    "model.compile(loss=\"mae\", optimizer=tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9))\n",
    "# ignore the epoch by epoch output by setting verbose = 0\n",
    "history = model.fit(dataset,epochs=200,verbose=0)\n",
    "model.summary()\n",
    "\n",
    "loss = history.history['loss']\n",
    "epochs = range(len(loss))\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
